To carry out the 3D map of a Martian rock, the coordinates of the points which are projected by the LED on its surface need to be determined on the images acquired by the camera. They are obtained by detecting the points thanks to their color, by distinguishing them, and by applying the centroid algorithm to get their center. First, the color detection method will be addressed, before focusing on the differentiation and the centroiding.

\subsection{Color Detection}

To increase the robustness of the algorithm, the color of the light beams was chosen to have the maximum contrast with the color of the rock. Mars having a reddish color, a bright green light seemed adapted. Moreover, as it is explained in the scene analysis part, the CCD sensor is more sensitive to the wavelength corresponding to the green (around 520 nm). Thus, the color detection algorithm aims to find bright green spots in the camera video stream in real time. It is implemented in C with OpenCV library. 

For unit testing, the image analysis is performed by extracting frames from the video stream obtained thanks to the computer webcam. These images are processed very quickly, which allows a real time analysis. The details of the method employed are given below. For the explanation, the figures are obtained for the detection of a blue box (the H value was changed for this purpose), but the algorithm works similarly for green light beams.

To begin with, a low-pass filter is applied to the image to handle to smooth it and to reduce the noise. Once achieved, the image is converted from RGB to HSV (see Theory Section) in order to get a better control of the colors and intensities of the image pixels. The range of the HSV parameters can be determined to isolate the bright green pixels from the rest of the image. The equivalent hue is between 60\degree  and 180\degree, taking a large scope to be sure to isolate the good color component. As the luminous points will be bright, the range of the saturation and the luminosity should be chosen around the upper values. To adjust the values, several trackbars are added to the image, see figure \ref{fig:trackbars} and the modifications are made in real time. All the values under the thresholds defined are irrelevant. Thus the pixels of the binary image which corresponds to the HSV thresholding are set to 1 (white) for the zone where the light green points are recognized and to 0 (black) for the background, the rest of the image. This image is then easier to deal with because it is only composed of two values, black and white. Erosions and dilations can be applied respectively to separate connected light beams and remove noise or to fill holes in the points. The number or erosions and dilations are selected thanks to trackbars once again (figure \ref{fig:trackbars}) and they are computed with the OpenCV functions with a 3x3 rectangular structuring element. The size of the kernel of the low-pass filter is also controllable with the trackbars. 

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{fig/trackbars.png}
  \caption{Trackbars for the threshold of the filter, the HSV parameters and the number of erosion and dilation}
  \label{fig:trackbars}
\end{figure}

The calibration is carried out through the trackbars and the result is displayed on the video stream with the addition of contours to facilitate the tuning (figure \ref{fig:contours}). \textit{cvFindContours} is used to search the contour of the light spots. The detection is based on the same principle that it has been seen during the exercises in the lab. The algorithm looks for the first pixel belonging to the point and then follows the contour pixel by pixel. Once the threshold of the low-pass filter, the HSV values and the number of erosion and dilation are established, the calibration is completed. For the unit testing with the blue box, we will only focus on the color detection and the centroiding. Thus, no erosion nor dilation will be added and the filter kernel is set to one pixel, which does nothing. 

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{fig/contours.png}
  \caption{Contours of the detected object}
  \label{fig:contours}
\end{figure}

In order to compute the centroid of the beams spots, it is needed to retrieve their intensity. To this end, the color detection should return an image with the light points in HSV color on a black background. To do so, the binary image is multiplied by the HSV image. The multiplication is explained by the pseudo code below. 

\begin{algorithmic}
\Function{multiplication}{binary image, color image}
	\For{each pixel in the binary image}
		\If {the pixel value is white}
    			\State $pixelHSVColor \gets$ the color of the pixel in the HSV image
    			\State the color of the pixel in the final image $\gets pixelHSVColor$
		\Else
        			\State the color of the pixel in the final image $\gets$ black color
		\EndIf
	\EndFor
\EndFunction
\end{algorithmic}

The final image obtained is shown figure \ref{fig:finalImage}. Each white pixel corresponding to 1 in the binary image is replaced in the final image by the HSV value of the original image and the background is set to black like in the binary image.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.6]{fig/finalImage.png}
  \caption{Object detected in HSV color system on a black background}
  \label{fig:finalImage}
\end{figure}

\subsection{Beams Distinction}
In the previous part, a unit test was performed with a blue box to check the efficiency of the color detection algorithm. However, the aim is to project a grid of 10 x 10 green points on a rock. Once they are extracted from the rest of the image with the same technique described above, they need to be separated. Indeed, for now, we only have the coordinates of the pixels assumed to belong to the beam spots. To look for their center of mass, the pixels which constitute each point have to be gathered. 

It should be noticed that, as no pattern other than an unicolour grid is used to carry out the depth mapping, the green points cannot be distinguished from each others. Thus, this implementation does not take into account the position of each beam spot, which could be problematic if the deformation is too important. Indeed, if a point is slightly linked to another or if it has moved and is a bit above or under the others, it is still possible to retrieve its original position. It is only if the change is too important that it will be very complicated to get the correct location of each beam spot. To solve this problem, different patterns as described in the Structured Light Theory Section have to be projected on the rock, which would assure that no spot has been switched. Nevertheless, this method requires more material, power consumption and is dismissed in a first approach. 

To differentiate each point, a recursive method was implemented using C++ which consists to isolate a point using neighbouring pixels. The pseudo code explaining the technique can be found below:

\begin{algorithmic}
\Function{findPointRec}{table, pixel, information to scan the image}\\
\Comment{table contains the coordinates of the pixel belonging to the current point}
		\If {the pixel value is is not black}
    			\State add to $table$ the $pixel$ coordinates and its value
    			\State the $pixel$ value in the image $\gets$ 0
			\State findPointRec(table, left neighbouring pixel, image information)
			\State findPointRec(table, right neighbouring pixel, image information)
			\State findPointRec(table, above neighbouring pixel, image information)
			\State findPointRec(table, below neighbouring pixel, image information))
		\EndIf
\EndFunction
\end{algorithmic}

The image is scanned and it is assumed that a new point begins each time that a green pixel is detected. A table to collect the coordinates of pixels belonging to the new beam spot is created and given in parameter to the recursive method. The first pixel position is added to the table, its value is set to 0 in the image and the recursive method is repeated while neighbouring pixels are found. At the end, a 2D table is obtained where the rows correspond to the spots and the columns to each pixel belonging to the point. The centroiding algorithm can then be processed to determine the center of mass of each spot and compute their distance to the camera. We can note that, for now, the points are separated but if they were not perfectly aligned on the projection, the rows of the 2D table are not in the same order than the original projected points.