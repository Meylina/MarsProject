Traditional cameras only take 2D pictures, with horizontal and vertical information. To reproduce the human vision useful in various applications such the stabilisation of a Martian rover harm in front of a rock to be analysed, it is often needed to add the depth data to get 3D coordinates of each pixel of the images. The depth mapping, also called 3D surface measurement or 3D imaging techniques etc, is carried out by two non-contact major methods as specified by \cite{sansoni2009state}: \say{projecting (in the active form) or acquiring (in the passive form) electromagnetic energy onto/from an object followed by recording the transmitted or reflected energy}. They can be divided into two categories, the non-optical and optical sensing. The first one generally uses acoustic and electromagnetic sensors to determine the distance from the system to the object, by assessing the duration of a round trip of a pulse. In the latter the light permits to get the depth. We will focus on this last technique as it will be more convenient to achieve it on Mars. Active optical sensing uses an additional light source where as passive optical sensing works only with the irradiance reaching the scene and the radiance of the object. 

\subsection{Principal Techniques Based on Image Analysis}
We will address only methods using image analysis as this is the purpose of the course.

Stereo vision and Photogrammetry systems are passive. The principle of the first is based on the utilisation of two or more cameras to record the scene.  The reconstruction of the 3D is carried out by solving a correspondance problem with the identification of patterns in two or several images. The photogrammetry method calculates the depth by taking pictures from different points of view of a scene, through a camera preliminary calibrated. Both techniques demand a high computing power. Thus they are dismissed since the Rover cannot provide too much power to the embedded processor. 

In active optical sensing, laser triangulators and structured light can be distinguished. They share the same approach: a beam or a pattern is projected through a laser source towards the object and its position on the image acquired by the camera is measured. Then, the distance from the camera to the object (the depth) is found by applying a triangulation technique. The difference between the two methods is that it is needed to scan the whole object for the laser triangulators one. Indeed, only a beam or a laser strip reaches it. The experiment has to be repeated several times to have an exhaustive 3D map. Regarding the structured light approach, unique patterns are projected simultaneously which allows to obtain the depth data at once if the object is small but in any case, it reduces a lot the number of acquisitions compared to the laser triangulators. According to \cite{tuto}, the patterns can be in 3D but they are rare. 2D patterns are the most common and easier to implement. Depending on the scene motion, the structured light method can be classified into multi-shot and single-shot. If the scene is moving, the acquisition time must be short and thus, one picture should be enough to get the desired information. On the other hand, if no constraint is given, several pictures can be taken and a pattern adapted to sequential can be selected. In our case, the scene does not move but the robot arm needs to be stabilised and could move. That is why only single-shot structured light will be considered.

In the future part of the report, the implementation and experiments will be performed through laser triangulators techniques. They are easier to set up but will only give us a partial depth map. To obtain a complete 3D map in a single shot, structured light should then be applied to the martian rock. A method that could be carried out then is described below. 